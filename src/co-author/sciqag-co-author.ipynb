{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13153596,"sourceType":"datasetVersion","datasetId":8333927},{"sourceId":13261509,"sourceType":"datasetVersion","datasetId":8403641}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\n\n# Read JSON file\ndef read_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        return json.load(file)\n\n# Extract titles and summaries\ndef extract_titles_and_summaries(json_data):\n    titles = [item.get(\"title\",\"\") for item in json_data]\n    abstracts = [item.get(\"abstract\",\"\") for item in json_data]\n    return titles, abstracts\n\n# File path (update with actual file path)\nfile_path = \"/kaggle/input/hepth-metadata-file/abs_metadata.json\"\n\ndata = read_json_file(file_path)\ntitles, abstracts = extract_titles_and_summaries(data)\n\n# Print results\n# for title, summary in titles_summaries:\n#     print(f\"Title: {title}\\nSummary: {summary}\\n\")\n\nlen(titles), len(abstracts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:09:39.628158Z","iopub.execute_input":"2025-10-04T16:09:39.628507Z","iopub.status.idle":"2025-10-04T16:09:40.057101Z","shell.execute_reply.started":"2025-10-04T16:09:39.628476Z","shell.execute_reply":"2025-10-04T16:09:40.055777Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"(29555, 29555)"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import gensim\ndata = abstracts\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(data))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:09:40.058172Z","iopub.execute_input":"2025-10-04T16:09:40.058492Z","iopub.status.idle":"2025-10-04T16:09:55.538500Z","shell.execute_reply.started":"2025-10-04T16:09:40.058461Z","shell.execute_reply":"2025-10-04T16:09:55.537699Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:09:55.539363Z","iopub.execute_input":"2025-10-04T16:09:55.539974Z","iopub.status.idle":"2025-10-04T16:10:08.720919Z","shell.execute_reply.started":"2025-10-04T16:09:55.539947Z","shell.execute_reply":"2025-10-04T16:10:08.719820Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import nltk\n# nltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n# Check if stopwords are accessible\nstop_words = stopwords.words('english')\nprint(stop_words)\n\n# Define functions for stopwords, bigrams, trigrams and lemmatization\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:10:08.723104Z","iopub.execute_input":"2025-10-04T16:10:08.723362Z","iopub.status.idle":"2025-10-04T16:10:09.505502Z","shell.execute_reply.started":"2025-10-04T16:10:08.723343Z","shell.execute_reply":"2025-10-04T16:10:09.504428Z"}},"outputs":[{"name":"stdout","text":"['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from gensim.utils import simple_preprocess\nimport spacy\n\n# NLTK Stop words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n\n# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_trigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:10:09.506424Z","iopub.execute_input":"2025-10-04T16:10:09.506777Z","iopub.status.idle":"2025-10-04T16:13:24.994379Z","shell.execute_reply.started":"2025-10-04T16:10:09.506725Z","shell.execute_reply":"2025-10-04T16:13:24.993392Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import gensim.corpora as corpora\n\n# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\nprint(id2word)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:13:24.995362Z","iopub.execute_input":"2025-10-04T16:13:24.996172Z","iopub.status.idle":"2025-10-04T16:13:27.172939Z","shell.execute_reply.started":"2025-10-04T16:13:24.996144Z","shell.execute_reply":"2025-10-04T16:13:27.171567Z"}},"outputs":[{"name":"stdout","text":"Dictionary<16326 unique tokens: ['airy', 'algebraic', 'aspect', 'behaviour', 'character']...>\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from gensim.models.ldamodel import LdaModel\nfrom gensim import corpora\n\n# dictionary = corpora.Dictionary(texts)\nlda_model = LdaModel.load(\"/kaggle/input/sciqagtopic-model-output/lda_model/model_lda_100_6.model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:13:27.173857Z","iopub.execute_input":"2025-10-04T16:13:27.175141Z","iopub.status.idle":"2025-10-04T16:13:27.210698Z","shell.execute_reply.started":"2025-10-04T16:13:27.175103Z","shell.execute_reply":"2025-10-04T16:13:27.209540Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"new_doc = \"This is a new document to infer topic distribution from.\"\nnew_doc_tokens = new_doc.lower().split()  # Apply your actual preprocessing here\nnew_doc_bow = id2word.doc2bow(new_doc_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:13:27.211584Z","iopub.execute_input":"2025-10-04T16:13:27.211874Z","iopub.status.idle":"2025-10-04T16:13:27.216878Z","shell.execute_reply.started":"2025-10-04T16:13:27.211850Z","shell.execute_reply":"2025-10-04T16:13:27.215828Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from gensim.models import Nmf\n\nnmf = Nmf.load(f\"/kaggle/input/sciqagtopic-model-output/nmf_model/model_nmf_100_17.model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:13:27.218056Z","iopub.execute_input":"2025-10-04T16:13:27.218367Z","iopub.status.idle":"2025-10-04T16:13:27.263037Z","shell.execute_reply.started":"2025-10-04T16:13:27.218347Z","shell.execute_reply":"2025-10-04T16:13:27.261945Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Assume your model and dictionary are already loaded\n# nmf = Nmf.load(\"/kaggle/working/model_nmf_100_10.model\")\n# id2word = ... (your original Dictionary)\n\n# New document\nnew_doc = \"deep learning in computer vision applications\"\n\n# Step 1: Preprocess (tokenize, lowercase, remove stopwords, etc.)\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import remove_stopwords\n\ntokens = simple_preprocess(remove_stopwords(new_doc))\n\n# Step 2: Convert to BoW\nbow_vector = id2word.doc2bow(tokens)\n\n# Step 3: Infer topics\ntopics = nmf.get_document_topics(bow_vector, minimum_probability=0.00)\nprint(topics)\n# Output inferred topics\nfor topic_id, prob in topics:\n    print(f\"Topic {topic_id}: Probability = {prob:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:13:27.264313Z","iopub.execute_input":"2025-10-04T16:13:27.264636Z","iopub.status.idle":"2025-10-04T16:13:27.273317Z","shell.execute_reply.started":"2025-10-04T16:13:27.264606Z","shell.execute_reply":"2025-10-04T16:13:27.272559Z"}},"outputs":[{"name":"stdout","text":"[(0, 0.017756917854348808), (4, 0.09999500069294077), (6, 0.2269878932152492), (8, 0.09574597512918757), (10, 0.18604006085773608), (11, 0.291873557487767), (12, 0.055104131658698416), (16, 0.02649646310407202)]\nTopic 0: Probability = 0.0178\nTopic 4: Probability = 0.1000\nTopic 6: Probability = 0.2270\nTopic 8: Probability = 0.0957\nTopic 10: Probability = 0.1860\nTopic 11: Probability = 0.2919\nTopic 12: Probability = 0.0551\nTopic 16: Probability = 0.0265\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"topic_distribution = lda_model.get_document_topics(new_doc_bow)\nprint(topic_distribution)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:19:04.584711Z","iopub.execute_input":"2025-10-04T16:19:04.585166Z","iopub.status.idle":"2025-10-04T16:19:04.592550Z","shell.execute_reply.started":"2025-10-04T16:19:04.585137Z","shell.execute_reply":"2025-10-04T16:19:04.591526Z"}},"outputs":[{"name":"stdout","text":"[(0, 0.17805512), (1, 0.063830435), (2, 0.18805428), (3, 0.3780252), (4, 0.027414372), (5, 0.16462056)]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"dense_dist = lda_model.get_document_topics(new_doc_bow, minimum_probability=0.0)\ndense_vector = [prob for topic_id, prob in dense_dist]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:19:06.421629Z","iopub.execute_input":"2025-10-04T16:19:06.422021Z","iopub.status.idle":"2025-10-04T16:19:06.428445Z","shell.execute_reply.started":"2025-10-04T16:19:06.421994Z","shell.execute_reply":"2025-10-04T16:19:06.427158Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"dense_vector","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:19:10.078229Z","iopub.execute_input":"2025-10-04T16:19:10.079121Z","iopub.status.idle":"2025-10-04T16:19:10.085036Z","shell.execute_reply.started":"2025-10-04T16:19:10.079088Z","shell.execute_reply":"2025-10-04T16:19:10.084153Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[0.17804936, 0.063830435, 0.18805426, 0.37803096, 0.027414372, 0.1646206]"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"for data_item in data:\n    abstract = data_item\n    # print(abstract)\n    new_doc_tokens = abstract.lower().split()  # Apply your actual preprocessing here\n    new_doc_bow = id2word.doc2bow(new_doc_tokens)\n    # dense_dist = lda_model.get_document_topics(new_doc_bow, minimum_probability=0.0)\n    # dense_vector = [prob for topic_id, prob in dense_dist]\n    num_topics = nmf.num_topics\n    topics_sparse = dict(nmf.get_document_topics(bow_vector, minimum_probability=0.0))\n    full_probs = [topics_sparse.get(i, 0.0) for i in range(num_topics)]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:26:31.523263Z","iopub.execute_input":"2025-10-04T16:26:31.523662Z","iopub.status.idle":"2025-10-04T16:26:54.685024Z","shell.execute_reply.started":"2025-10-04T16:26:31.523641Z","shell.execute_reply":"2025-10-04T16:26:54.683984Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"datas[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:44:03.424894Z","iopub.execute_input":"2025-06-04T06:44:03.425227Z","iopub.status.idle":"2025-06-04T06:44:03.434473Z","shell.execute_reply.started":"2025-06-04T06:44:03.425198Z","shell.execute_reply":"2025-06-04T06:44:03.433077Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'id': 'http://arxiv.org/abs/hep-ph/9907233v3',\n 'guidislink': True,\n 'link': 'http://arxiv.org/abs/hep-ph/9907233v3',\n 'updated': '2000-02-24T05:29:39Z',\n 'updated_parsed': [2000, 2, 24, 5, 29, 39, 3, 55, 0],\n 'published': '1999-07-05T14:17:06Z',\n 'published_parsed': [1999, 7, 5, 14, 17, 6, 0, 186, 0],\n 'title': 'The Radiative Decay of Vector Mesons',\n 'title_detail': {'type': 'text/plain',\n  'language': None,\n  'base': '',\n  'value': 'The Radiative Decay of Vector Mesons'},\n 'summary': \"In this paper, radiative decays $\\\\rho^0 \\\\to \\\\pi^+\\\\pi^-\\\\gamma,\\n\\\\pi^0\\\\pi^0\\\\gamma$ ,$\\\\phi \\\\to K^+K^-\\\\gamma, K^0 \\\\bar{K^0}\\\\gamma$ are studied\\nsystematically in the U(3)$_L\\\\timesU(3)_R$ chiral theory of mesons. The\\ntheoretical differential spectrum with respect to photon energy and branch\\nratio for $\\\\rho^0 \\\\to \\\\pi^+\\\\pi^-\\\\gamma$ agree well with the experimental data.\\nDifferential spectrums and branch ratios for $\\\\rho^0 \\\\to \\\\pi^0\\\\pi^0\\\\gamma, \\\\phi\\n\\\\to K^+ K^-\\\\gamma,\\\\phi \\\\to K^0\\\\bar{K^0}\\\\gamma$ are predicted. The process $\\\\phi\\n\\\\to K^0 \\\\bar{K^0} \\\\gamma$ is relevant to precision measurment of CP-violation\\nparameters in the kaon systerm at a $\\\\phi$-factory. We give a complete estimate\\nof the branch ratio for this decay process by including scalar resonance $f_0,\\na_0$ poles, nonresonant smooth amplitude and an abnormal parity process with\\n$K^*$ pole which hasn't been considered before. We conclude that processes with\\nintermediate $K^*$ do not pose a potential background problem for $\\\\phi\\\\to\\nK^0\\\\bar{K}^0$ CP violation experiments.\",\n 'summary_detail': {'type': 'text/plain',\n  'language': None,\n  'base': '',\n  'value': \"In this paper, radiative decays $\\\\rho^0 \\\\to \\\\pi^+\\\\pi^-\\\\gamma,\\n\\\\pi^0\\\\pi^0\\\\gamma$ ,$\\\\phi \\\\to K^+K^-\\\\gamma, K^0 \\\\bar{K^0}\\\\gamma$ are studied\\nsystematically in the U(3)$_L\\\\timesU(3)_R$ chiral theory of mesons. The\\ntheoretical differential spectrum with respect to photon energy and branch\\nratio for $\\\\rho^0 \\\\to \\\\pi^+\\\\pi^-\\\\gamma$ agree well with the experimental data.\\nDifferential spectrums and branch ratios for $\\\\rho^0 \\\\to \\\\pi^0\\\\pi^0\\\\gamma, \\\\phi\\n\\\\to K^+ K^-\\\\gamma,\\\\phi \\\\to K^0\\\\bar{K^0}\\\\gamma$ are predicted. The process $\\\\phi\\n\\\\to K^0 \\\\bar{K^0} \\\\gamma$ is relevant to precision measurment of CP-violation\\nparameters in the kaon systerm at a $\\\\phi$-factory. We give a complete estimate\\nof the branch ratio for this decay process by including scalar resonance $f_0,\\na_0$ poles, nonresonant smooth amplitude and an abnormal parity process with\\n$K^*$ pole which hasn't been considered before. We conclude that processes with\\nintermediate $K^*$ do not pose a potential background problem for $\\\\phi\\\\to\\nK^0\\\\bar{K}^0$ CP violation experiments.\"},\n 'authors': [{'name': 'T. -L. Zhuang'},\n  {'name': 'X. -J. Wang'},\n  {'name': 'M. -L. Yan'}],\n 'author_detail': {'name': 'M. -L. Yan'},\n 'author': 'M. -L. Yan',\n 'arxiv_doi': '10.1103/PhysRevD.62.053007',\n 'links': [{'title': 'doi',\n   'href': 'http://dx.doi.org/10.1103/PhysRevD.62.053007',\n   'rel': 'related',\n   'type': 'text/html'},\n  {'href': 'http://arxiv.org/abs/hep-ph/9907233v3',\n   'rel': 'alternate',\n   'type': 'text/html'},\n  {'title': 'pdf',\n   'href': 'http://arxiv.org/pdf/hep-ph/9907233v3',\n   'rel': 'related',\n   'type': 'application/pdf'}],\n 'arxiv_comment': 'Revtex file, 12 pages, 9 eps figures',\n 'arxiv_journal_ref': 'Phys.Rev. D62 (2000) 053007',\n 'arxiv_primary_category': {'term': 'hep-ph',\n  'scheme': 'http://arxiv.org/schemas/atom'},\n 'tags': [{'term': 'hep-ph',\n   'scheme': 'http://arxiv.org/schemas/atom',\n   'label': None}],\n 'topic_dis': [0.07535813509651315,\n  0.044252196162134924,\n  0.010281809610639412,\n  0.0,\n  0.018990276526449138,\n  0.004524991499263155,\n  0.214218333548413,\n  0.0,\n  0.0,\n  0.2672737622800575,\n  0.3306402614580234,\n  0.009745985533785225,\n  0.0,\n  0.0,\n  0.0005351395433550339,\n  0.0,\n  0.02417910874136598]}"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"import json\nimport itertools\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom collections import defaultdict\n\ncoauthor_topics = defaultdict(list)\n\nfor paper in datas:\n    authors = [a[\"name\"] for a in paper.get(\"authors\", []) if \"name\" in a]\n    topic_dis = paper.get(\"topic_dis\",[0,0,0,0])\n    \n    if not topic_dis or len(authors) < 2:\n        continue\n\n    for author1, author2 in itertools.combinations(sorted(authors), 2):\n        pair = tuple(sorted([author1, author2]))\n        coauthor_topics[pair].append(topic_dis)\n\nedges = {}\nfor pair, topic_dis_list in coauthor_topics.items():\n    topic_array = np.array(topic_dis_list)\n    avg_topic = topic_array.mean(axis=0)\n    edges[pair] = avg_topic\n\nG = nx.Graph()\n\nauthor_topics = defaultdict(list)\n\nfor paper in datas:\n    topic_dis = paper.get(\"topic_dis\")\n    authors = [a.get(\"name\") for a in paper.get(\"authors\", []) if \"name\" in a]\n\n    if topic_dis is None or not authors:\n        continue\n\n    for author in authors:\n        author_topics[author].append(topic_dis)\n\nfor author, distributions in author_topics.items():\n    \n    avg_topic = np.mean(distributions, axis=0)\n    G.add_node(author, weight=avg_topic.tolist())\n\n\nfor (author1, author2), topic_vec in edges.items():\n    G.add_edge(author1, author2, weight=topic_vec.tolist())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:44:03.435563Z","iopub.execute_input":"2025-06-04T06:44:03.435872Z","iopub.status.idle":"2025-06-04T06:44:07.155424Z","shell.execute_reply.started":"2025-06-04T06:44:03.435845Z","shell.execute_reply":"2025-06-04T06:44:07.154546Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"author_articles = {}\n\nfor article in datas:\n\n    authors = article.get('authors', [])\n    article_id = article.get('id')\n\n    for author in authors:\n        name = author.get('name', 'Unknown Author')\n        if name not in author_articles:\n            author_articles[name] = []\n        author_articles[name].append(article_id)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T01:56:26.196261Z","iopub.execute_input":"2025-06-02T01:56:26.196530Z","iopub.status.idle":"2025-06-02T01:56:26.285779Z","shell.execute_reply.started":"2025-06-02T01:56:26.196512Z","shell.execute_reply":"2025-06-02T01:56:26.285093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\nwith open(\"graph.pkl\", \"wb\") as f:\n    pickle.dump(G, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T06:44:07.163163Z","iopub.execute_input":"2025-06-04T06:44:07.163509Z","iopub.status.idle":"2025-06-04T06:44:07.453710Z","shell.execute_reply.started":"2025-06-04T06:44:07.163463Z","shell.execute_reply":"2025-06-04T06:44:07.452479Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"for author, ids in author_articles.items():\n    author_articles[author] = [id.replace(\"http://arxiv.org/abs/\", \"\") for id in ids]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T01:58:22.613815Z","iopub.execute_input":"2025-06-02T01:58:22.614620Z","iopub.status.idle":"2025-06-02T01:58:22.665965Z","shell.execute_reply.started":"2025-06-02T01:58:22.614544Z","shell.execute_reply":"2025-06-02T01:58:22.664987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"author_articles","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T01:58:26.956739Z","iopub.execute_input":"2025-06-02T01:58:26.957023Z","iopub.status.idle":"2025-06-02T01:58:27.048677Z","shell.execute_reply.started":"2025-06-02T01:58:26.957001Z","shell.execute_reply":"2025-06-02T01:58:27.047553Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nwith open(\"author_papers.json\", \"w\", encoding=\"utf-8\") as f:\n    json.dump(author_articles, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T01:59:23.672748Z","iopub.execute_input":"2025-06-02T01:59:23.673023Z","iopub.status.idle":"2025-06-02T01:59:23.728953Z","shell.execute_reply.started":"2025-06-02T01:59:23.672997Z","shell.execute_reply":"2025-06-02T01:59:23.728219Z"}},"outputs":[],"execution_count":null}]}
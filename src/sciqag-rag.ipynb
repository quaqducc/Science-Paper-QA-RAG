{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13153596,"sourceType":"datasetVersion","datasetId":8333927},{"sourceId":13261509,"sourceType":"datasetVersion","datasetId":8403641}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install and Import Libs","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade gensim==4.3.2 smart_open==6.4.0\n!pip install langchain langchain-community chromadb sentence-transformers gensim nltk tqdm --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:44:59.557989Z","iopub.execute_input":"2025-10-04T16:44:59.558380Z","iopub.status.idle":"2025-10-04T16:47:56.978370Z","shell.execute_reply.started":"2025-10-04T16:44:59.558348Z","shell.execute_reply":"2025-10-04T16:47:56.976933Z"}},"outputs":[{"name":"stdout","text":"Collecting gensim==4.3.2\n  Downloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\nCollecting smart_open==6.4.0\n  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (1.26.4)\nRequirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.2) (1.15.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim==4.3.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim==4.3.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim==4.3.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim==4.3.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim==4.3.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->gensim==4.3.2) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->gensim==4.3.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->gensim==4.3.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->gensim==4.3.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.5->gensim==4.3.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.5->gensim==4.3.2) (2024.2.0)\nDownloading gensim-4.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: smart_open, gensim\n  Attempting uninstall: smart_open\n    Found existing installation: smart-open 7.1.0\n    Uninstalling smart-open-7.1.0:\n      Successfully uninstalled smart-open-7.1.0\n  Attempting uninstall: gensim\n    Found existing installation: gensim 4.3.3\n    Uninstalling gensim-4.3.3:\n      Successfully uninstalled gensim-4.3.3\nSuccessfully installed gensim-4.3.2 smart_open-6.4.0\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.32.1 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.32.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.1 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\njupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip uninstall -y protobuf google-cloud-storage google-cloud-core google-cloud-automl\n!pip install protobuf==3.20.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:47:56.984099Z","iopub.execute_input":"2025-10-04T16:47:56.984418Z","iopub.status.idle":"2025-10-04T16:48:06.714418Z","shell.execute_reply.started":"2025-10-04T16:47:56.984364Z","shell.execute_reply":"2025-10-04T16:48:06.712960Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: protobuf 6.32.1\nUninstalling protobuf-6.32.1:\n  Successfully uninstalled protobuf-6.32.1\nFound existing installation: google-cloud-storage 2.19.0\nUninstalling google-cloud-storage-2.19.0:\n  Successfully uninstalled google-cloud-storage-2.19.0\nFound existing installation: google-cloud-core 2.4.3\nUninstalling google-cloud-core-2.4.3:\n  Successfully uninstalled google-cloud-core-2.4.3\nFound existing installation: google-cloud-automl 1.0.1\nUninstalling google-cloud-automl-1.0.1:\n  Successfully uninstalled google-cloud-automl-1.0.1\nCollecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-translate 3.12.1 requires google-cloud-core<3.0.0dev,>=1.4.4, which is not installed.\ngoogle-cloud-datastore 2.21.0 requires google-cloud-core<3.0.0,>=1.4.0, which is not installed.\nfirebase-admin 6.9.0 requires google-cloud-storage>=1.37.1, which is not installed.\ngoogle-cloud-spanner 3.55.0 requires google-cloud-core<3.0.0,>=1.4.4, which is not installed.\ngoogle-cloud-aiplatform 1.99.0 requires google-cloud-storage<3.0.0,>=1.32.0, which is not installed.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nbigframes 2.8.0 requires google-cloud-storage>=2.0.0, which is not installed.\ngoogle-cloud-firestore 2.21.0 requires google-cloud-core<3.0.0,>=1.4.1, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install --force-reinstall \"scipy==1.10.1\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:48:06.716853Z","iopub.execute_input":"2025-10-04T16:48:06.717210Z","iopub.status.idle":"2025-10-04T16:48:26.864324Z","shell.execute_reply.started":"2025-10-04T16:48:06.717176Z","shell.execute_reply":"2025-10-04T16:48:26.863094Z"}},"outputs":[{"name":"stdout","text":"Collecting scipy==1.10.1\n  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting numpy<1.27.0,>=1.19.5 (from scipy==1.10.1)\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, scipy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.3\n    Uninstalling scipy-1.15.3:\n      Successfully uninstalled scipy-1.15.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-aiplatform 1.99.0 requires google-cloud-storage<3.0.0,>=1.32.0, which is not installed.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nbigframes 2.8.0 requires google-cloud-storage>=2.0.0, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\nkaggle-environments 1.17.6 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\njax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nscikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nxarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\ncvxpy 1.6.6 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\njaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4 scipy-1.10.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os, re, json, pickle\nfrom tqdm import tqdm\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:48:26.865809Z","iopub.execute_input":"2025-10-04T16:48:26.866192Z","iopub.status.idle":"2025-10-04T16:48:28.753020Z","shell.execute_reply.started":"2025-10-04T16:48:26.866149Z","shell.execute_reply":"2025-10-04T16:48:28.751813Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Utility Function","metadata":{}},{"cell_type":"code","source":"STOPWORDS = set(stopwords.words('english'))\n\ndef read_json_file(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        return json.load(f)\n\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^a-z\\s]', ' ', text)\n    tokens = [w for w in word_tokenize(text) if w not in STOPWORDS and len(w) > 2]\n    return \" \".join(tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:48:28.754168Z","iopub.execute_input":"2025-10-04T16:48:28.754541Z","iopub.status.idle":"2025-10-04T16:48:28.765309Z","shell.execute_reply.started":"2025-10-04T16:48:28.754504Z","shell.execute_reply":"2025-10-04T16:48:28.764090Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Preprocess and NMF Model","metadata":{}},{"cell_type":"code","source":"import gensim\nimport gensim.corpora as corpora\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.models.nmf import Nmf\n\n\ndef preprocess_texts(texts):\n    lemmatizer = WordNetLemmatizer()\n    tokenized = [text.split() for text in texts]\n    bigram = Phrases(tokenized, min_count=5, threshold=100)\n    bigram_mod = Phraser(bigram)\n    tokenized = [bigram_mod[doc] for doc in tokenized]\n    data_lemmatized = [[lemmatizer.lemmatize(w) for w in doc] for doc in tokenized]\n    return data_lemmatized\n\ndef build_nmf_model(data_lemmatized, num_topics=20):\n    id2word = corpora.Dictionary(data_lemmatized)\n    corpus = [id2word.doc2bow(text) for text in data_lemmatized]\n    nmf_model = Nmf(corpus=corpus, num_topics=num_topics, id2word=id2word, random_state=42)\n    return nmf_model, id2word, corpus\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:48:28.767064Z","iopub.execute_input":"2025-10-04T16:48:28.767407Z","iopub.status.idle":"2025-10-04T16:48:29.892220Z","shell.execute_reply.started":"2025-10-04T16:48:28.767380Z","shell.execute_reply":"2025-10-04T16:48:29.890926Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Load Metadata","metadata":{}},{"cell_type":"code","source":"data_path = \"/kaggle/input/hepth-metadata-file/abs_metadata.json\"\npapers = read_json_file(data_path)\n\ntexts = [clean_text(p[\"abstract\"]) for p in papers]\nprint(f\"Loaded {len(texts)} abstracts.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:49:28.110251Z","iopub.execute_input":"2025-10-04T16:49:28.111030Z","iopub.status.idle":"2025-10-04T16:49:40.038509Z","shell.execute_reply.started":"2025-10-04T16:49:28.110992Z","shell.execute_reply":"2025-10-04T16:49:40.037531Z"}},"outputs":[{"name":"stdout","text":"Loaded 29555 abstracts.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# NMF TOPIC MODEL","metadata":{}},{"cell_type":"code","source":"import os\n\nos.makedirs(\"data\", exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:50:29.027302Z","iopub.execute_input":"2025-10-04T16:50:29.027898Z","iopub.status.idle":"2025-10-04T16:50:29.034513Z","shell.execute_reply.started":"2025-10-04T16:50:29.027868Z","shell.execute_reply":"2025-10-04T16:50:29.033310Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"data_lemmatized = preprocess_texts(texts)\nnmf_model, id2word, corpus = build_nmf_model(data_lemmatized, num_topics=20)\n\n# Lưu model\nos.makedirs(\"data\", exist_ok=True)\nwith open(\"data/nmf_model.pkl\", \"wb\") as f:\n    pickle.dump(nmf_model, f)\nwith open(\"data/id2word.pkl\", \"wb\") as f:\n    pickle.dump(id2word, f)\n\nprint(\"NMF model built successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:50:32.096472Z","iopub.execute_input":"2025-10-04T16:50:32.096888Z","iopub.status.idle":"2025-10-04T16:50:53.602184Z","shell.execute_reply.started":"2025-10-04T16:50:32.096857Z","shell.execute_reply":"2025-10-04T16:50:53.601017Z"}},"outputs":[{"name":"stdout","text":"NMF model built successfully!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# CREAT EMBEDDING AND CHROMADB","metadata":{}},{"cell_type":"code","source":"from langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\nembed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n\n# Tạo 2 collection: abstract và topic\ncollection = Chroma(\n    persist_directory=\"data/chroma_collection\",\n    embedding_function=embed_model\n)\ntopic_collection = Chroma(\n    persist_directory=\"data/chroma_topic_collection\",\n    embedding_function=embed_model\n)\n\nprint(\"ChromaDB collections ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:51:05.549620Z","iopub.execute_input":"2025-10-04T16:51:05.550918Z","iopub.status.idle":"2025-10-04T16:52:03.041729Z","shell.execute_reply.started":"2025-10-04T16:51:05.550734Z","shell.execute_reply":"2025-10-04T16:52:03.040410Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3202640238.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n2025-10-04 16:51:25.776859: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759596686.107944      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759596686.194482      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b12ec2cd4c26478cbb5ab1a3af62c3b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b660ae20fc7442f7aab2e416322d4462"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be1b3b67887942bbab44dda41745d00e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84a0bae5993d4517a3750d33720a7197"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c47aff20a27b4b539a6887d123881ede"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88babfa0a71c4d0d999750c18b5550ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c026b38903854a8d9f1649d57d8248f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3435732e20374009bc73828c0b2d1976"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"977bbbb254274941b45996913cd4f0ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da9ee7cd4db942739e7665f24d76251f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc9d90f5e881472ca18a636f768abe26"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/3202640238.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n  collection = Chroma(\n","output_type":"stream"},{"name":"stdout","text":"ChromaDB collections ready.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"\n# RETRIEVER SEMANTIC & TOPIC\n","metadata":{}},{"cell_type":"code","source":"\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\nclass TopicChunkedChromaRetriever:\n    def __init__(self, collection, topic_collection, nmf_model, id2word, embed_model,\n                 top_k_docs=5, chunk_size=500, chunk_overlap=50, top_k_chunks=5):\n        self.collection = collection\n        self.topic_collection = topic_collection\n        self.nmf_model = nmf_model\n        self.id2word = id2word\n        self.embed_model = embed_model\n        self.top_k_docs = top_k_docs\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.top_k_chunks = top_k_chunks\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap\n        )\n\n    def get_topic_vector(self, text):\n        tokens = text.lower().split()\n        bow = self.id2word.doc2bow(tokens)\n        num_topics = self.nmf_model.num_topics\n        topics_sparse = dict(self.nmf_model.get_document_topics(bow, minimum_probability=0.0))\n        dense_vector = [topics_sparse.get(i, 0.0) for i in range(num_topics)]\n        return dense_vector\n\n    def retrieve(self, query):\n        dense_vector = self.get_topic_vector(query)\n        results = self.topic_collection.query(\n            query_embeddings=[dense_vector],\n            n_results=50,\n            include=[\"documents\", \"ids\"]\n        )\n        ids = results[\"ids\"][0]\n\n        query_embedding = self.embed_model.embed_query(query)\n        results = self.collection.query(\n            query_embeddings=[query_embedding],\n            n_results=self.top_k_docs,\n            where={\"paper_id\": {\"$in\": ids}},\n            include=[\"documents\"]\n        )\n\n        return results[\"documents\"][0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:53:16.234880Z","iopub.execute_input":"2025-10-04T16:53:16.235221Z","iopub.status.idle":"2025-10-04T16:53:16.255216Z","shell.execute_reply.started":"2025-10-04T16:53:16.235199Z","shell.execute_reply":"2025-10-04T16:53:16.254082Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# LLM CHAIN + PROMPT","metadata":{}},{"cell_type":"code","source":"from langchain.prompts import ChatPromptTemplate\nfrom langchain_community.chat_models import ChatOpenAI\nfrom langchain.chains import LLMChain\n\ndef make_rag_prompt():\n    template = \"\"\"\n    You are a helpful academic assistant. Based on the retrieved documents below, answer the user's question.\n\n    === Retrieved Documents ===\n    {context}\n\n    === Question ===\n    {question}\n\n    Provide a concise and factual answer based on the context.\n    \"\"\"\n    return ChatPromptTemplate.from_template(template)\n\ndef build_rag_chain():\n    prompt = make_rag_prompt()\n    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.3)\n    chain = LLMChain(prompt=prompt, llm=llm)\n    return chain\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:53:21.644858Z","iopub.execute_input":"2025-10-04T16:53:21.647156Z","iopub.status.idle":"2025-10-04T16:53:21.886939Z","shell.execute_reply.started":"2025-10-04T16:53:21.647103Z","shell.execute_reply":"2025-10-04T16:53:21.885826Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Pipeline","metadata":{}},{"cell_type":"code","source":"retriever = TopicChunkedChromaRetriever(\n    collection=collection,\n    topic_collection=topic_collection,\n    nmf_model=nmf_model,\n    id2word=id2word,\n    embed_model=embed_model\n)\n\nrag_chain = build_rag_chain()\n\nquery = \"What are recent advances in diffusion models for image generation?\"\ndocs = retriever.retrieve(query)\ncontext = \"\\n\".join(docs)\n\nanswer = rag_chain.run({\"context\": context, \"question\": query})\nprint(\"=== ANSWER ===\\n\", answer)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}